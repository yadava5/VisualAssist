# Getting Started with Visual Assist

Learn how to set up and use Visual Assist for the first time.

@Metadata {
    @PageColor(green)
}

## Overview

Visual Assist provides three main operating modes for helping visually impaired users navigate their environment:

1. **Navigation Mode** — Real-time obstacle detection using LiDAR
2. **Text Reading Mode** — Point-and-read OCR with natural speech
3. **Object Awareness Mode** — AI-powered scene understanding

## First Launch

When you first launch Visual Assist:

1. **Grant Permissions** — The app requires camera, microphone, and speech recognition access
2. **Wait for Ready** — The app announces "Visual Assist ready" when fully loaded
3. **Select a Mode** — Tap any mode card or use voice commands

## Voice Commands

Control the app hands-free with voice commands:

| Command | Action |
|---------|--------|
| "Navigate" | Start obstacle detection |
| "Read text" | Begin text reading |
| "What's around me" | Describe surroundings |
| "Stop" | Stop current action |
| "Help" | List all commands |

## Haptic Feedback

Visual Assist uses distinct haptic patterns to communicate:

| Pattern | Meaning |
|---------|---------|
| Single tap | Action confirmed |
| Double pulse | Mode changed |
| Continuous vibration | Critical obstacle (<0.5m) |
| Triple pulse | Warning (0.5–1m) |

## Tips for Best Results

- **Hold steady** — Keep the phone at chest height, camera facing forward
- **Move slowly** — Allow the sensors time to scan the environment
- **Listen carefully** — Audio feedback provides important spatial information
- **Use headphones** — AirPods provide the best spatial audio experience

## See Also

- <doc:Accessibility>
