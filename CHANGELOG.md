# Changelog

All notable changes to VisualAssist will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Planned
- Apple Watch companion app
- Indoor mapping and floor plan recognition
- Currency and banknote identification
- Multi-language support for text reading

---

## [0.1.0-beta] - 2026-01-05

### Added
- **Navigation Mode** - Real-time obstacle detection using LiDAR depth sensing
- **Object Awareness Mode** - ML-powered object recognition with spatial audio cues
- **Text Reading Mode** - OCR-based text recognition with speech output
- **Voice Commands** - Hands-free control for all app features
- **Haptic Feedback** - Intuitive haptic patterns for proximity warnings
- **Spatial Audio** - 3D audio positioning for detected objects
- **Accessibility-First Design** - Full VoiceOver support and Dynamic Type
- **Settings Panel** - Customizable detection distances, speech rate, and feedback preferences

### Technical
- iOS 17.0+ support
- LiDAR-equipped iPhone requirement (12 Pro and newer)
- ARKit integration for depth processing
- Vision framework for text recognition
- Core ML for object detection
- Core Haptics for haptic feedback
- AVFoundation for camera and speech services

### Documentation
- Comprehensive README with setup instructions
- DocC documentation catalog
- Contributing guidelines
- Security policy
- Code of conduct

---

## Version History

| Version | Date | Status |
|---------|------|--------|
| 0.1.0-beta | 2026-01-05 | Current Beta |

---

[Unreleased]: https://github.com/yadava5/VisualAssist/compare/v0.1.0-beta...HEAD
[0.1.0-beta]: https://github.com/yadava5/VisualAssist/releases/tag/v0.1.0-beta
